{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e5e84a",
   "metadata": {},
   "source": [
    "The objective of this capstone project is to design and implement a fully local Retrieval-Augmented Generation (RAG) system using a desktop-hosted Large Language Model (LLM) such as Ollama, LM Studio, or GPT4All.\n",
    "\n",
    "The system ingests private documents (PDF or text), converts them into vector embeddings using Sentence Transformers, stores them in a local ChromaDB vector database, and enables users to query the document through a LangChain-based RAG agent.\n",
    "\n",
    "The solution ensures data privacy, offline inference, and accurate grounded responses by combining semantic retrieval with LLM reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ab433",
   "metadata": {},
   "source": [
    "1. Load private documents (PDF / TXT)\n",
    "2. Split documents into overlapping chunks\n",
    "3. Generate embeddings using Sentence Transformers\n",
    "4. Store embeddings in local ChromaDB\n",
    "5. Configure a local LLM (Ollama / LM Studio)\n",
    "6. Build a retriever over the vector store\n",
    "7. Inject retrieved context into a RAG prompt\n",
    "8. Generate grounded answers using the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffcc0f4",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "This section installs and imports all required libraries for building\n",
    "a fully local Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "Key components:\n",
    "- LangChain: Orchestration framework\n",
    "- Sentence Transformers: Embedding generation\n",
    "- ChromaDB: Local vector database\n",
    "- Ollama / LM Studio: Desktop LLM inference\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83023cd9",
   "metadata": {},
   "source": [
    "Step 1: Install Ollama (One-Time Setup)\n",
    "\n",
    "Note: This step is performed outside the notebook.\n",
    "\n",
    "Download and install Ollama from the official site:\n",
    "\n",
    "https://ollama.com\n",
    "\n",
    "\n",
    "After installation, verify:\n",
    "\n",
    "ollama --version\n",
    "\n",
    "ðŸ”¹ Step 2: Pull the Mistral Model\n",
    "ollama pull mistral\n",
    "\n",
    "Step 3: Run the Model Locally\n",
    "ollama run mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c0867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Nasir\\GENAI\\Level-4 Assignment\\Level-4 Assignment\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Hadi2\\AppData\\Local\\Temp\\ipykernel_22364\\351585580.py:14: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This module initializes a locally hosted LLM using Ollama or LM Studio.\n",
    "\n",
    "Why local LLM?\n",
    "- Ensures data privacy\n",
    "- Avoids external API costs\n",
    "- Enables offline inference\n",
    "\n",
    "The LLM is later used only for generation,\n",
    "while retrieval is handled by ChromaDB.\n",
    "\"\"\"\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"mistral\",\n",
    "    temperature=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30825981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 pages\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Loads private documents (PDF or TXT) from disk.\n",
    "\n",
    "The loader abstracts file format differences and converts\n",
    "each document into LangChain Document objects, preserving metadata\n",
    "such as page numbers for traceability.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "\n",
    "def load_documents(path: str):\n",
    "    if path.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(path)\n",
    "    else:\n",
    "        loader = TextLoader(path)\n",
    "    return loader.load()\n",
    "\n",
    "docs = load_documents(\"attention.pdf\")\n",
    "print(f\"Loaded {len(docs)} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b074c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks: 66\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Splits documents into overlapping chunks to balance:\n",
    "- Context preservation\n",
    "- Embedding quality\n",
    "- Retrieval accuracy\n",
    "\n",
    "Chunk overlap ensures that important information\n",
    "is not lost across chunk boundaries.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"Total Chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0afe6025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hadi2\\AppData\\Local\\Temp\\ipykernel_22364\\3340233381.py:16: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generates dense vector embeddings using a Sentence Transformer model.\n",
    "\n",
    "Why Sentence Transformers?\n",
    "- Lightweight and fast\n",
    "- High semantic similarity performance\n",
    "- Suitable for local execution\n",
    "\n",
    "Each chunk is converted into a numerical vector\n",
    "for similarity-based retrieval.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7196cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hadi2\\AppData\\Local\\Temp\\ipykernel_22364\\2865966172.py:20: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vector_db.persist()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Stores embeddings in a local ChromaDB vector database.\n",
    "\n",
    "Advantages:\n",
    "- Persistent local storage\n",
    "- Fast similarity search\n",
    "- No cloud dependency\n",
    "\n",
    "This database acts as the long-term memory\n",
    "for the RAG system.\n",
    "\"\"\"\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chroma_store\"\n",
    ")\n",
    "\n",
    "vector_db.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db5802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates a semantic retriever over the vector database.\n",
    "\n",
    "The retriever fetches the top-k most relevant chunks\n",
    "based on cosine similarity, which are later injected\n",
    "into the LLM prompt as context.\n",
    "\"\"\"\n",
    "\n",
    "retriever = vector_db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e70939d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defines a Retrieval-Augmented Generation prompt.\n",
    "\n",
    "The prompt explicitly instructs the LLM to:\n",
    "- Answer strictly using retrieved context\n",
    "- Avoid hallucinations\n",
    "- Return 'Not found in the document' if context is missing\n",
    "\n",
    "This is critical for trustworthy RAG behavior.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a highly accurate AI assistant.\n",
    "Answer the question strictly using the context provided.\n",
    "If the answer is not in the context, say \"Not found in the document\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db3a5edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Builds a modern LangChain Expression Language (LCEL) pipeline.\n",
    "\n",
    "Pipeline steps:\n",
    "1. User query\n",
    "2. Context retrieval from ChromaDB\n",
    "3. Context formatting\n",
    "4. Prompt injection\n",
    "5. LLM reasoning\n",
    "\n",
    "LCEL ensures composability, clarity, and future extensibility.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53610b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Executes the RAG pipeline for user queries.\n",
    "\n",
    "This function acts as the user-facing interface\n",
    "for querying private documents and receiving\n",
    "grounded, context-aware answers.\n",
    "\"\"\"\n",
    "\n",
    "def ask_rag(question: str):\n",
    "    return rag_chain.invoke(question)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0727d43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The encoder and decoder stacks in the Transformer model architecture play crucial roles. The encoder, composed of a stack of N = 6 identical layers, takes an input sequence and transforms it into a contextualized representation that can be understood by the model. Each layer has two sub-layers: the first is a multi-head self-attention mechanism that allows the model to understand the relationships between different parts of the input sequence, and the second is a simple, position-wise fully connected feed-forward network that helps the model learn more complex patterns in the data. A residual connection and layer normalization are employed around each sub-layer to help with training and improve the model's performance.\n",
      "\n",
      "On the other hand, the decoder also has a stack of identical layers, similar to the encoder. Its role is to generate an output sequence based on the contextualized representation provided by the encoder. The decoder uses the same sub-layers as the encoder but processes the information in a reverse order, starting from the first position and moving towards the last. This allows it to generate sequences of any length, making it suitable for tasks like translation or text summarization where the output sequence length can vary.\n"
     ]
    }
   ],
   "source": [
    "response = ask_rag(\n",
    "    \" Explain the role of the encoder and decoder stacks.\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a158f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The context provided does not detail the difference between multi-head attention and single-head attention. However, in general, multi-head attention allows a model to attend to information from different positions within an input sequence simultaneously, while single-head attention only attends to one position at a time. This allows multi-head attention to capture more complex patterns and relationships within the data.\n"
     ]
    }
   ],
   "source": [
    "response = ask_rag(\n",
    "    \" How does multi-head attention differ from single-head attention?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28375b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The provided context does not explicitly state why removing recurrence improves scalability in sequence modeling. However, it implies that the model is auto-regressive [10], which means at each step, the model consumes the previously generated symbols as additional input when generating the next. This structure might contribute to improved scalability compared to models with recurrence, as it potentially reduces the number of computations required for long sequences by not having to maintain a hidden state over time. However, this is an inference based on the context and not a direct statement.\n"
     ]
    }
   ],
   "source": [
    "response = ask_rag(\n",
    "    \" Why does removing recurrence improve scalability in sequence modeling?\"\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
