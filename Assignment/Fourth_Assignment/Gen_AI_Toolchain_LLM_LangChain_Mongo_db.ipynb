{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253b98e9",
   "metadata": {},
   "source": [
    "# Assignment 4 â€“ GenAI Toolchain with LCEL, MongoDB \n",
    "# =====================================================================\n",
    "#\n",
    "# \n",
    "# Objective:\n",
    "# Build a complete GenAI toolchain using:\n",
    "# - MongoDB Atlas (NoSQL knowledge store)\n",
    "# - LangChain LCEL (RunnableSequence)\n",
    "# - Lightweight Open-Source Embeddings\n",
    "# - LLM Reasoning Layer (google/flan-t5-large)\n",
    "#\n",
    "# The system ingests employee profiles into MongoDB and answers\n",
    "# structured + semantic queries in a deterministic, hallucination-free way.\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Install Dependencies (Run Once)\n",
    "# ---------------------------------------------------------------------\n",
    "# !pip install -U langchain langchain-community pymongo sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9e2f17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Nasir\\GENAI\\Level-4 Assignment\\Level-4 Assignment\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 2. Imports\n",
    "# ---------------------------------------------------------------------\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import MongoDBAtlasVectorSearch\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1047d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 3. Configuration\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "MONGO_URI = "MONGO DB URI,
    "\n",
    "DB_NAME = \"genai_db\"\n",
    "COLLECTION_NAME = \"employees\"\n",
    "VECTOR_INDEX = \"vector_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3d7f575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 4. MongoDB Connection\n",
    "# ---------------------------------------------------------------------\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[DB_NAME]\n",
    "collection = db[COLLECTION_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af91b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 5. Employee Dataset (Knowledge Base)\n",
    "# ---------------------------------------------------------------------\n",
    "employees = [\n",
    "    {\"name\": \"Nitin Kulkarni\", \"role\": \"AI Solutions Architect\", \"skills\": \"LangChain, RAG, LLM Architecture, Vector Databases\", \"experience\": \"12 years\", \"location\": \"Bangalore\"},\n",
    "    {\"name\": \"Sara Khan\", \"role\": \"GenAI Engineer\", \"skills\": \"LangChain, Prompt Engineering, HuggingFace, FAISS\", \"experience\": \"3 years\", \"location\": \"Hyderabad\"},\n",
    "    {\"name\": \"Arjun Patel\", \"role\": \"AI Research Engineer\", \"skills\": \"Transformers, Deep Learning, PyTorch\", \"experience\": \"6 years\", \"location\": \"Ahmedabad\"},\n",
    "    {\"name\": \"Priya Menon\", \"role\": \"ML Engineer\", \"skills\": \"ML Pipelines, Feature Engineering\", \"experience\": \"5 years\", \"location\": \"Bangalore\"},\n",
    "    {\"name\": \"Vikram Rao\", \"role\": \"Cloud Architect\", \"skills\": \"AWS, DevOps, MLOps\", \"experience\": \"10 years\", \"location\": \"Bangalore\"},\n",
    "    {\"name\": \"Kiran Desai\", \"role\": \"AI Platform Engineer\", \"skills\": \"LangChain, RAG, Kubernetes\", \"experience\": \"8 years\", \"location\": \"Bangalore\"},\n",
    "    {\"name\": \"Meera Joshi\", \"role\": \"NLP Engineer\", \"skills\": \"NLP, RAG Pipelines\", \"experience\": \"6 years\", \"location\": \"Mumbai\"},\n",
    "    {\"name\": \"Farhan Ali\", \"role\": \"GenAI Engineer\", \"skills\": \"LangChain, RAG, Vector Search\", \"experience\": \"5 years\", \"location\": \"Hyderabad\"},\n",
    "    {\"name\": \"Lakshmi Krishnan\", \"role\": \"Principal ML Scientist\", \"skills\": \"GenAI Research, Advanced ML\", \"experience\": \"15 years\", \"location\": \"Bangalore\"},\n",
    "    {\"name\": \"Sneha Kulkarni\", \"role\": \"AI Business Analyst\", \"skills\": \"AI Use Cases, GenAI Adoption\", \"experience\": \"6 years\", \"location\": \"Pune\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f800cf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hadi2\\AppData\\Local\\Temp\\ipykernel_17296\\453587048.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 6. Embeddings (Lightweight & Open Source)\n",
    "# ---------------------------------------------------------------------\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0adc6121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents in MongoDB: 64\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 7. Ingest Data into MongoDB Atlas\n",
    "# ---------------------------------------------------------------------\n",
    "texts = []\n",
    "for emp in employees:\n",
    "    texts.append(f\"\"\"\n",
    "EMPLOYEE_NAME: {emp['name']}\n",
    "ROLE: {emp['role']}\n",
    "SKILLS: {emp['skills']}\n",
    "EXPERIENCE: {emp['experience']}\n",
    "LOCATION: {emp['location']}\n",
    "\"\"\")\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch.from_texts(\n",
    "    texts=texts,\n",
    "    embedding=embeddings,\n",
    "    collection=collection,\n",
    "    index_name=VECTOR_INDEX\n",
    ")\n",
    "\n",
    "print(\"Documents in MongoDB:\", collection.count_documents({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88702521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 8. Fetch-All Runnable (Deterministic Retrieval)\n",
    "# ---------------------------------------------------------------------\n",
    "def fetch_all_docs(_):\n",
    "    return [Document(page_content=d[\"text\"]) for d in collection.find({}, {\"text\": 1})]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f27ffbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "C:\\Users\\Hadi2\\AppData\\Local\\Temp\\ipykernel_17296\\952511785.py:14: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=hf_pipeline)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 9. LLM Reasoning Layer (google/flan-t5-large)\n",
    "# ---------------------------------------------------------------------\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "hf_pipeline = pipeline(\n",
    "    task=\"text2text-generation\",\n",
    "    model=\"google/flan-t5-large\",\n",
    "    max_new_tokens=64,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "\n",
    "\n",
    "def llm_reasoning(inputs):\n",
    "    query = inputs[\"query\"]\n",
    "    profile = inputs[\"profile\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Answer with ONLY YES or NO.\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Employee Profile:\n",
    "{profile}\n",
    "\n",
    "Is this employee relevant?\n",
    "\"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt).strip().upper()\n",
    "    return response.startswith(\"YES\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c238ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 10. LCEL Filtering with LLM Reasoning\n",
    "# ---------------------------------------------------------------------\n",
    "def extract_name(text):\n",
    "    for line in text.splitlines():\n",
    "        if line.startswith(\"EMPLOYEE_NAME:\"):\n",
    "            return line.split(\":\", 1)[1].strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "def filter_with_llm(inputs):\n",
    "    query = inputs[\"query\"]\n",
    "    docs = inputs[\"docs\"]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for doc in docs:\n",
    "        name = extract_name(doc.page_content)\n",
    "        if not name:\n",
    "            continue\n",
    "\n",
    "        decision = llm_reasoning({\n",
    "            \"query\": query,\n",
    "            \"profile\": doc.page_content\n",
    "        })\n",
    "\n",
    "        if decision:\n",
    "            results.append(name)\n",
    "\n",
    "    return list(dict.fromkeys(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f60880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 11. Final Pure LCEL RunnableSequence\n",
    "# ---------------------------------------------------------------------\n",
    "lcel_chain = (\n",
    "    {\n",
    "        \"query\": RunnablePassthrough(),\n",
    "        \"docs\": RunnableLambda(fetch_all_docs)\n",
    "    }\n",
    "    | RunnableLambda(filter_with_llm)\n",
    "    | RunnableLambda(lambda names: \"\\n\".join(names))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e59853b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUERY: Who has experience with LangChain and RAG?\n",
      "Sneha Kulkarni\n",
      "Nitin Kulkarni\n",
      "Sara Khan\n",
      "Priya Menon\n",
      "Kiran Desai\n",
      "Meera Joshi\n",
      "Amit Gupta\n",
      "Neha Reddy\n",
      "Farhan Ali\n",
      "\n",
      "QUERY: List employees from Bangalore\n",
      "Nitin Kulkarni\n",
      "Priya Menon\n",
      "Vikram Rao\n",
      "Kiran Desai\n",
      "Pooja Malhotra\n",
      "Lakshmi Krishnan\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 12. Test Queries (Final Output)\n",
    "# ---------------------------------------------------------------------\n",
    "queries = [\n",
    "    \"Who has experience with LangChain and RAG?\",\n",
    "    \"List employees from Bangalore\"\n",
    "   \n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nQUERY: {q}\")\n",
    "    print(lcel_chain.invoke(q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc325475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUERY: Which employees are suitable for building a GenAI platform?\n",
      "Sneha Kulkarni\n",
      "Nitin Kulkarni\n",
      "Sara Khan\n",
      "Arjun Patel\n",
      "Priya Menon\n",
      "Vikram Rao\n",
      "Kiran Desai\n",
      "Meera Joshi\n",
      "Amit Gupta\n",
      "Neha Reddy\n",
      "Pooja Malhotra\n",
      "Farhan Ali\n",
      "Lakshmi Krishnan\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 12. Test Queries (Final Output)\n",
    "# ---------------------------------------------------------------------\n",
    "queries = [\n",
    "    \"Which employees are suitable for building a GenAI platform?\",\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\nQUERY: {q}\")\n",
    "    print(lcel_chain.invoke(q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
